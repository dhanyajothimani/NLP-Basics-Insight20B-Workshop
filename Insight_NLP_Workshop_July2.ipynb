{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insight_NLP_Workshop_July2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcMTyzkUbMc6",
        "colab_type": "text"
      },
      "source": [
        "#**Introduction of Natural Language Processing (NLP)**\n",
        "\n",
        "At the end of this workshop, you will be able to understand\n",
        "\n",
        "*   the basic NLP pipeline\n",
        "*   the different word embeddings and their pros and cons \n",
        "*   how to use custom stop words\n",
        "*   the implementation of NLP in Python \n",
        "*   how to use Google Colab (- if you are not already familiar :) )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BreSY3Shgnd9",
        "colab_type": "text"
      },
      "source": [
        "## **What is NLP?** \n",
        "\n",
        "- Sub field of Computer Science and Artificial Intelligence that focuses on interactions between computer and human (natural) languages \n",
        "- Application of machine learning (ML) and deep learning (DL) algorithms to text and speech (datasets). \n",
        "- Applications: Speech recognition, machine translation, spam detection, auto complete/next word suggestion, chat bot etc. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQHqntZo8yi9",
        "colab_type": "text"
      },
      "source": [
        "# **NLP Pipeline**\n",
        "\n",
        "Here, NLP pipeline refers to the pre-processing steps that should be applied on the text data before proceeding towards the machine learning aspect of the model. \n",
        "\n",
        "For example, the **objective** of a project is identification of e-mails as spam (or non-spam). \n",
        "\n",
        "1.   Identification of *type of ML problem*: Classification (using text data)\n",
        "2.   ML algorithms: Multinomial naive bayes, Logistic Regression and Support Vector Machine\n",
        "\n",
        "Great! We have an idea about the type of problem and what possible ML algorithms to use. But before that, how do we process the text data?  \n",
        "\n",
        "Here is an outline of the steps that we could use for processig the text data:\n",
        "\n",
        "### **Text Pre-processing** \n",
        "\n",
        "1.   Spell check (- depending on the context)\n",
        "1.   Sentence Tokenization\n",
        "2.   Word Tokenization\n",
        "3.   Conversion to lower case\n",
        "4.   Lexicon Normalization: Lemmatization and Stemming \n",
        "5.   Removal of puncatuations and stop words (and numbers - depending on the context)\n",
        "6.   Parts-of-speech (POS) tagging \n",
        "7.   Creation of n-grams \n",
        "\n",
        "### **Exploratory Analysis**\n",
        "\n",
        "1.   Word Cloud\n",
        "2.   Distribution of data with respect to each class \n",
        "\n",
        "### **Word Embeddings** \n",
        "\n",
        "1.   Bag-of-Words (BoW)\n",
        "2.   Term Frequency (TF)\n",
        "3.   Term Frequency - Inverse Document Frequency (TF - IDF)\n",
        "4.   Pre-trained (Neural) Word Embeddings \n",
        "\n",
        "> * Word level embeddings: Word2Vec and Glove \n",
        "> * Character level embeddings: ELMo and Flair \n",
        "\n",
        "We will explore each of these topics using a dataset. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTkwt96ZLEDR",
        "colab_type": "text"
      },
      "source": [
        "## **Resources in Python**\n",
        "\n",
        "* [NLTK](https://www.nltk.org/): Natural Language Tool Kit \n",
        "* [spaCy](https://spacy.io/)\n",
        "* [Gensim](https://github.com/RaRe-Technologies/gensim)\n",
        "-----\n",
        "* [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
        "* [CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n",
        "* [polyglot](https://polyglot.readthedocs.io/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHrqJsFK9caD",
        "colab_type": "text"
      },
      "source": [
        "### **NLP Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6nfRcgwIURU",
        "colab_type": "text"
      },
      "source": [
        "#### *Spell Check*\n",
        " To check for spelling errors and to get possible alternatives for the misspelled words. \n",
        "\n",
        "\n",
        "*   Using `autocorrect` module \n",
        "*   Using `pyspellchecker` module: It uses a *Levenshtein Distance algorithm* to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results.\n",
        "*   Using `textblob` module: returns two values – a recommended correction for this word, and a confidence score associated with the correction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WitPGm0qKZ7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b4b7e3aa-3b58-488d-a5f8-3650da11adb7"
      },
      "source": [
        "#!pip install autocorrect\n",
        "#Using autocorrect module \n",
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "print(spell('caaaar'))\n",
        "print(spell('mussage'))\n",
        "print(spell('survice'))\n",
        "print(spell('hte'))\n",
        "print(spell(\"Let is check whehter spel check works hree\"))  #Correct sentence: Let us check whether spell check works here\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "caesar\n",
            "message\n",
            "service\n",
            "the\n",
            "Let is check whether spell check works here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMyNPkjxLTVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "4acb2882-32f9-43d1-c01d-d803cf6f32e9"
      },
      "source": [
        "#Using pyspellchecker \n",
        "#!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['let', 'us', 'wlak','on','the','groun'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "group\n",
            "{'grout', 'ground', 'groin', 'grown', 'groan', 'group'}\n",
            "walk\n",
            "{'flak', 'walk', 'weak'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0imoi0pRhXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "10dd1f54-1f20-4a23-f51c-a4fbae650e6f"
      },
      "source": [
        "#!pip install textblob\n",
        "from textblob import Word\n",
        "from textblob import TextBlob\n",
        "\n",
        "word = Word('personell')\n",
        "print(word.spellcheck())\n",
        "\n",
        "b = TextBlob(\"I havv goood speling!\")\n",
        "print(b.correct())"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('personal', 0.65), ('personally', 0.2642857142857143), ('peroneal', 0.06428571428571428), ('personnel', 0.014285714285714285), ('personen', 0.007142857142857143)]\n",
            "I have good spelling!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7_PCNYyIa1w",
        "colab_type": "text"
      },
      "source": [
        "#### *Sentence Tokenization*\n",
        "\n",
        "To break paragraphs into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZx61sXtUAAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e8fc4ec4-0567-4a3d-cd01-d0350ed6541e"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"\"\"\"Oh, Marilla, looking forward to things is half the pleasure of them,\" exclaimed Anne. \n",
        "        \"You mayn’t get the things themselves; but nothing can prevent you from having the fun of looking forward to them. \n",
        "        Mrs. Lynde says, 'Blessed are they who expect nothing for they shall not be disappointed.' \n",
        "        But I think it would be worse to expect nothing than to be disappointed.\"\"\"\n",
        "\n",
        "#Text from Anne of Green Gables\n",
        "\n",
        "tokenized_text=sent_tokenize(text)\n",
        "\n",
        "print(tokenized_text)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\"Oh, Marilla, looking forward to things is half the pleasure of them,\" exclaimed Anne.', '\"You mayn’t get the things themselves; but nothing can prevent you from having the fun of looking forward to them.', \"Mrs. Lynde says, 'Blessed are they who expect nothing for they shall not be disappointed.'\", 'But I think it would be worse to expect nothing than to be disappointed.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc9SXQkcIiFN",
        "colab_type": "text"
      },
      "source": [
        "#### *Word Tokenization*\n",
        "\n",
        "To break sentences into words (or tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ7IDcwmVROK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4ba1d64e-a389-46d9-8bab-ce1127f58ada"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['``', 'Oh', ',', 'Marilla', ',', 'looking', 'forward', 'to', 'things', 'is', 'half', 'the', 'pleasure', 'of', 'them', ',', \"''\", 'exclaimed', 'Anne', '.', '``', 'You', 'mayn', '’', 't', 'get', 'the', 'things', 'themselves', ';', 'but', 'nothing', 'can', 'prevent', 'you', 'from', 'having', 'the', 'fun', 'of', 'looking', 'forward', 'to', 'them', '.', 'Mrs.', 'Lynde', 'says', ',', \"'Blessed\", 'are', 'they', 'who', 'expect', 'nothing', 'for', 'they', 'shall', 'not', 'be', 'disappointed', '.', \"'\", 'But', 'I', 'think', 'it', 'would', 'be', 'worse', 'to', 'expect', 'nothing', 'than', 'to', 'be', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O534gEh4ImGu",
        "colab_type": "text"
      },
      "source": [
        "#### *Conversion to lower case*\n",
        "\n",
        "Converting the text (in upper case or sentence case) to lower case. \n",
        "\n",
        "1.   Helps to maintain consistency of expected output.\n",
        "2.   Maintains uniformity among different cases - easiers to search. For instance, searching for \"Canada\" may not yield results - if the text is \"canada\"\n",
        "3.   Often times, word embeddings might perform poorly \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUfJ96viWS9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1a7ecfbc-f1d4-487a-a5d4-5e39f752cb05"
      },
      "source": [
        "lower_case_text = text.lower()\n",
        "print(lower_case_text)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"oh, marilla, looking forward to things is half the pleasure of them,\" exclaimed anne. \n",
            "        \"you mayn’t get the things themselves; but nothing can prevent you from having the fun of looking forward to them. \n",
            "        mrs. lynde says, 'blessed are they who expect nothing for they shall not be disappointed.' \n",
            "        but i think it would be worse to expect nothing than to be disappointed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTazaia3Ip-t",
        "colab_type": "text"
      },
      "source": [
        "#### *Removal of punctuations and stop words*\n",
        "\n",
        "Stop words such as \"a\", \"the\", \"in\" etc do not add meaning in the text analysis and are considered as noise in the data. Hence, they should be removed. Also, it is advisable to remove the punctuations and numbers (depending upon the context). \n",
        "\n",
        "P.S.: Sometimes, removing the punctuation might distort the meaning of the word. For instance, in the sentence \"you're good\" would return \"youre good\" - where the meaning of the sentence is lost "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FKtIiQUZECT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "086a14a3-3500-4e88-b8a7-a100b9176a6e"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n",
        "\n",
        "\n",
        "filtered_sent=[]\n",
        "\n",
        "for w in tokenized_word:\n",
        "    if w not in stop_words:\n",
        "        filtered_sent.append(w)\n",
        "\n",
        "print(\"Tokenized Sentence:\",tokenized_word)\n",
        "print(\"Filterd Sentence:\",filtered_sent)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'too', 't', 'were', 'because', 'wouldn', 'can', 'yourselves', 'is', 'hadn', 'through', 'now', \"aren't\", 'or', 're', \"mustn't\", \"doesn't\", 'was', \"that'll\", 'i', 'down', 'we', 'isn', \"weren't\", 'some', 'by', 'so', \"couldn't\", 'very', 'been', 'these', 'how', 'himself', 'o', 'don', \"mightn't\", \"she's\", 'from', 'do', 'haven', 'the', 'ain', 'have', \"needn't\", 'being', 'while', 'until', 'of', 'has', 'themselves', 'at', 'not', 'are', 'myself', 'on', \"it's\", 'about', \"isn't\", 'just', 'her', 'their', 'mightn', 'then', 'where', 'than', 'they', 'nor', 'them', 'off', 'against', 'only', 'and', 'y', 've', 'weren', 'above', 'for', 'which', 'your', 'itself', 'ma', 'he', 'd', 'with', 'does', 'ourselves', 'yourself', 'an', 'what', 'below', 'other', 'such', 'aren', 'you', 'under', \"hadn't\", \"you'd\", 'once', 'own', \"should've\", 'between', 'after', 'up', 'be', 'any', 'will', 'in', 'again', 'she', \"you're\", 'here', 'this', 'when', 'won', \"shan't\", 'did', 'should', 'me', 'needn', 'as', \"didn't\", 'its', 'my', \"haven't\", 'before', 'it', 'but', 's', 'theirs', 'hers', 'into', 'further', \"wouldn't\", 'why', 'who', 'him', 'out', 'll', 'couldn', 'm', 'mustn', \"you've\", 'few', 'that', 'had', 'am', \"hasn't\", 'having', \"shouldn't\", 'same', 'doing', 'a', \"won't\", 'our', 'his', 'herself', 'doesn', 'yours', 'didn', 'no', 'shouldn', 'each', 'those', 'ours', 'most', 'whom', 'to', 'wasn', \"wasn't\", 'during', 'hasn', 'all', 'more', \"you'll\", 'if', 'both', \"don't\", 'shan', 'over', 'there'}\n",
            "Tokenized Sentence: ['``', 'Oh', ',', 'Marilla', ',', 'looking', 'forward', 'to', 'things', 'is', 'half', 'the', 'pleasure', 'of', 'them', ',', \"''\", 'exclaimed', 'Anne', '.', '``', 'You', 'mayn', '’', 't', 'get', 'the', 'things', 'themselves', ';', 'but', 'nothing', 'can', 'prevent', 'you', 'from', 'having', 'the', 'fun', 'of', 'looking', 'forward', 'to', 'them', '.', 'Mrs.', 'Lynde', 'says', ',', \"'Blessed\", 'are', 'they', 'who', 'expect', 'nothing', 'for', 'they', 'shall', 'not', 'be', 'disappointed', '.', \"'\", 'But', 'I', 'think', 'it', 'would', 'be', 'worse', 'to', 'expect', 'nothing', 'than', 'to', 'be', 'disappointed', '.']\n",
            "Filterd Sentence: ['``', 'Oh', ',', 'Marilla', ',', 'looking', 'forward', 'things', 'half', 'pleasure', ',', \"''\", 'exclaimed', 'Anne', '.', '``', 'You', 'mayn', '’', 'get', 'things', ';', 'nothing', 'prevent', 'fun', 'looking', 'forward', '.', 'Mrs.', 'Lynde', 'says', ',', \"'Blessed\", 'expect', 'nothing', 'shall', 'disappointed', '.', \"'\", 'But', 'I', 'think', 'would', 'worse', 'expect', 'nothing', 'disappointed', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rSZqGyrbc1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "58ec7a86-22c0-4d22-9262-492829264bd5"
      },
      "source": [
        "#Removal of punctuations\n",
        "\n",
        "words_no_punkt = [word for word in tokenized_word if word.isalpha()]\n",
        "print(words_no_punkt[:100])"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Oh', 'Marilla', 'looking', 'forward', 'to', 'things', 'is', 'half', 'the', 'pleasure', 'of', 'them', 'exclaimed', 'Anne', 'You', 'mayn', 't', 'get', 'the', 'things', 'themselves', 'but', 'nothing', 'can', 'prevent', 'you', 'from', 'having', 'the', 'fun', 'of', 'looking', 'forward', 'to', 'them', 'Lynde', 'says', 'are', 'they', 'who', 'expect', 'nothing', 'for', 'they', 'shall', 'not', 'be', 'disappointed', 'But', 'I', 'think', 'it', 'would', 'be', 'worse', 'to', 'expect', 'nothing', 'than', 'to', 'be', 'disappointed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QbgBuIrInay",
        "colab_type": "text"
      },
      "source": [
        "#### *Lexicon normalization: Lemmatization and Stemming*\n",
        "\n",
        "*Stemming*: Reduces the words to their root form by removing the derivational affixes. For instance, connection, connected, connecting word reduce to a common word \"connect\". \n",
        "\n",
        "> 1.   Porter Stemmer: Uses a set of five rules to remove the suffix (also known as suffix stripping)\n",
        "> 2.   Lancaster Stemmer: Uses an iterative algorithm that uses a set of 120 rules to remove the suffixes. The algorithm tries to find an applicable rule by the last character of the word. Each rule specifies a replacement or deletion of a terminal character. It continues till it could find no such rule. Another stopping criterion - a word starting with a vowel and with only two characters left; or a word starting with a consonant and with only three characters left. \n",
        "The process repeats till it meets one of the stopping criteria. However, this could lead to over stemming. \n",
        "\n",
        "*Lemmatization*: Reduces words to their base word (or called lemmas). Since it is based on morphological analysis, lemmatization is better than stemming. Stemmer ignores the context of the word. For example, lemma of \"better\" is \"good\" - which is missed by the stemmer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfHcNrHadYzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2d2d97fb-4693-45e6-d3e3-6096727081ad"
      },
      "source": [
        "\n",
        "\n",
        "from nltk import PorterStemmer, LancasterStemmer  #Other stemmer is LancasterStemmer; PorterStemmer is commonly used since it is simple and fast to use \n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "\n",
        "#A list of words to be stemmed\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Porter Stemmer      lancaster Stemmer   \n",
            "friend              friend              friend              \n",
            "friendship          friendship          friend              \n",
            "friends             friend              friend              \n",
            "friendships         friendship          friend              \n",
            "stabil              stabil              stabl               \n",
            "destabilize         destabil            dest                \n",
            "misunderstanding    misunderstand       misunderstand       \n",
            "railroad            railroad            railroad            \n",
            "moonlight           moonlight           moonlight           \n",
            "football            footbal             footbal             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFOalmgje2ID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e8c506b-e5dc-48fa-aacc-8adf2d42aa16"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in words_no_punkt:\n",
        "    print (\"{0:20}{1:20}\".format(word,lem.lemmatize(word)))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "Oh                  Oh                  \n",
            "Marilla             Marilla             \n",
            "looking             looking             \n",
            "forward             forward             \n",
            "to                  to                  \n",
            "things              thing               \n",
            "is                  is                  \n",
            "half                half                \n",
            "the                 the                 \n",
            "pleasure            pleasure            \n",
            "of                  of                  \n",
            "them                them                \n",
            "exclaimed           exclaimed           \n",
            "Anne                Anne                \n",
            "You                 You                 \n",
            "mayn                mayn                \n",
            "t                   t                   \n",
            "get                 get                 \n",
            "the                 the                 \n",
            "things              thing               \n",
            "themselves          themselves          \n",
            "but                 but                 \n",
            "nothing             nothing             \n",
            "can                 can                 \n",
            "prevent             prevent             \n",
            "you                 you                 \n",
            "from                from                \n",
            "having              having              \n",
            "the                 the                 \n",
            "fun                 fun                 \n",
            "of                  of                  \n",
            "looking             looking             \n",
            "forward             forward             \n",
            "to                  to                  \n",
            "them                them                \n",
            "Lynde               Lynde               \n",
            "says                say                 \n",
            "are                 are                 \n",
            "they                they                \n",
            "who                 who                 \n",
            "expect              expect              \n",
            "nothing             nothing             \n",
            "for                 for                 \n",
            "they                they                \n",
            "shall               shall               \n",
            "not                 not                 \n",
            "be                  be                  \n",
            "disappointed        disappointed        \n",
            "But                 But                 \n",
            "I                   I                   \n",
            "think               think               \n",
            "it                  it                  \n",
            "would               would               \n",
            "be                  be                  \n",
            "worse               worse               \n",
            "to                  to                  \n",
            "expect              expect              \n",
            "nothing             nothing             \n",
            "than                than                \n",
            "to                  to                  \n",
            "be                  be                  \n",
            "disappointed        disappointed        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaOfY2_LjVgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1efba00-85e5-403d-c70a-23a5bc186b2c"
      },
      "source": [
        "for word in words_no_punkt:\n",
        "    print (\"{0:20}{1:20}\".format(word,lem.lemmatize(word, pos=\"v\")))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oh                  Oh                  \n",
            "Marilla             Marilla             \n",
            "looking             look                \n",
            "forward             forward             \n",
            "to                  to                  \n",
            "things              things              \n",
            "is                  be                  \n",
            "half                half                \n",
            "the                 the                 \n",
            "pleasure            pleasure            \n",
            "of                  of                  \n",
            "them                them                \n",
            "exclaimed           exclaim             \n",
            "Anne                Anne                \n",
            "You                 You                 \n",
            "mayn                mayn                \n",
            "t                   t                   \n",
            "get                 get                 \n",
            "the                 the                 \n",
            "things              things              \n",
            "themselves          themselves          \n",
            "but                 but                 \n",
            "nothing             nothing             \n",
            "can                 can                 \n",
            "prevent             prevent             \n",
            "you                 you                 \n",
            "from                from                \n",
            "having              have                \n",
            "the                 the                 \n",
            "fun                 fun                 \n",
            "of                  of                  \n",
            "looking             look                \n",
            "forward             forward             \n",
            "to                  to                  \n",
            "them                them                \n",
            "Lynde               Lynde               \n",
            "says                say                 \n",
            "are                 be                  \n",
            "they                they                \n",
            "who                 who                 \n",
            "expect              expect              \n",
            "nothing             nothing             \n",
            "for                 for                 \n",
            "they                they                \n",
            "shall               shall               \n",
            "not                 not                 \n",
            "be                  be                  \n",
            "disappointed        disappoint          \n",
            "But                 But                 \n",
            "I                   I                   \n",
            "think               think               \n",
            "it                  it                  \n",
            "would               would               \n",
            "be                  be                  \n",
            "worse               worse               \n",
            "to                  to                  \n",
            "expect              expect              \n",
            "nothing             nothing             \n",
            "than                than                \n",
            "to                  to                  \n",
            "be                  be                  \n",
            "disappointed        disappoint          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A68wvoTdIq70",
        "colab_type": "text"
      },
      "source": [
        "#### *Parts-of-speech (POS) tagging*\n",
        "\n",
        "Identifies the grammar groups (such as noun, verb, pronoun etc) in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1vCzb_nkKQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "201bc564-c4c6-453e-a698-a5aed03b02b7"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(words_no_punkt[:15]) #IN- preposition; NNP- noun, proper, singular; VB - Verb; RB - adverb; TO: \"to\" as preposition or infinitive marker; \n",
        "#VBG: verb, present participle or gerund; VBD: verb, past tense\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Oh', 'UH'),\n",
              " ('Marilla', 'NNP'),\n",
              " ('looking', 'VBG'),\n",
              " ('forward', 'RB'),\n",
              " ('to', 'TO'),\n",
              " ('things', 'NNS'),\n",
              " ('is', 'VBZ'),\n",
              " ('half', 'PDT'),\n",
              " ('the', 'DT'),\n",
              " ('pleasure', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('them', 'PRP'),\n",
              " ('exclaimed', 'VBD'),\n",
              " ('Anne', 'NNP'),\n",
              " ('You', 'PRP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ge3gsosJGvD",
        "colab_type": "text"
      },
      "source": [
        "#### *Creation of n-grams*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-qIjXxFFnnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j57FI8DVitF",
        "colab_type": "text"
      },
      "source": [
        "## **Classification of Real and Fake News**\n",
        "\n",
        "To classify whether a news article is fake or real.\n",
        " \n",
        "Data source: [Kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset?select=Fake.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaudFkHQbEMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "042c00bd-6f6e-4b57-8624-6c72563e2d8e"
      },
      "source": [
        "import warnings \n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import os \n",
        "from google.colab import files\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "#nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#Feature extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "\n",
        "#Classifiers\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC,  LinearSVC\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "#Performance metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-e2PUXshrZt",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4462803c-efdb-4b3c-bf00-51b8424d2499"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7cc78d19-4aad-417e-ad72-a52d558a266d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7cc78d19-4aad-417e-ad72-a52d558a266d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Fake.csv to Fake.csv\n",
            "Saving True.csv to True.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_qCpCRKiWPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake_data = pd.read_csv(\"Fake.csv\")\n",
        "real_data = pd.read_csv(\"True.csv\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esY5K_mUlagr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f3a744cc-fc6e-4c37-9035-1f014fbcca0b"
      },
      "source": [
        "fake_data.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...               date\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...  ...  December 31, 2017\n",
              "1   Drunk Bragging Trump Staffer Started Russian ...  ...  December 31, 2017\n",
              "2   Sheriff David Clarke Becomes An Internet Joke...  ...  December 30, 2017\n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...  December 29, 2017\n",
              "4   Pope Francis Just Called Out Donald Trump Dur...  ...  December 25, 2017\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6W4rhNIljdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d142b97e-b072-44c7-efa1-ed175ccc6cb3"
      },
      "source": [
        "real_data.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                date\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...  December 31, 2017 \n",
              "1  U.S. military to accept transgender recruits o...  ...  December 29, 2017 \n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...  December 31, 2017 \n",
              "3  FBI Russia probe helped by Australian diplomat...  ...  December 30, 2017 \n",
              "4  Trump wants Postal Service to charge 'much mor...  ...  December 29, 2017 \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6IHKreHmSGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "d32c1ac9-c8fa-4cbb-9e7a-da813a1b10ac"
      },
      "source": [
        "#Print the number of missing values in both dataframes: real_data and fake_data\n",
        "print(\"For fake_data\")\n",
        "print(fake_data.isnull().sum())\n",
        "\n",
        "print(\"For real_data\")\n",
        "print(real_data.isnull().sum())"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For fake_data\n",
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "dtype: int64\n",
            "For real_data\n",
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU9SGpqinvZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5e7c80f8-0906-4bf2-dfc1-0871e6a5dbdd"
      },
      "source": [
        "fake_data[\"label\"] = 1\n",
        "real_data[\"label\"] = 0\n",
        "print(fake_data.head())\n",
        "print(real_data.head())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               title  ... label\n",
            "0   Donald Trump Sends Out Embarrassing New Year’...  ...     1\n",
            "1   Drunk Bragging Trump Staffer Started Russian ...  ...     1\n",
            "2   Sheriff David Clarke Becomes An Internet Joke...  ...     1\n",
            "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...     1\n",
            "4   Pope Francis Just Called Out Donald Trump Dur...  ...     1\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "                                               title  ... label\n",
            "0  As U.S. budget fight looms, Republicans flip t...  ...     0\n",
            "1  U.S. military to accept transgender recruits o...  ...     0\n",
            "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...     0\n",
            "3  FBI Russia probe helped by Australian diplomat...  ...     0\n",
            "4  Trump wants Postal Service to charge 'much mor...  ...     0\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtaMkVMApkng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "bbd0578d-8f60-4905-9aca-ae8d5bc65cb9"
      },
      "source": [
        "#Combine the both fake_data and real_data; shuffle the data; and split them into training and test dataset\n",
        "\n",
        "print(\"Dimension of Fake News:\", fake_data.shape)\n",
        "print(\"Dimension of Real News:\", real_data.shape)\n",
        "fake_real_data = pd.concat([fake_data,real_data])\n",
        "print(\"Dimension of combined dataset:\", fake_real_data.shape)\n",
        "fake_real_data = fake_real_data.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension of Fake News: (23481, 5)\n",
            "Dimension of Real News: (21417, 5)\n",
            "Dimension of combined dataset: (44898, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p39LVc_VviqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8300283-c284-4042-f4ab-33342de29a6c"
      },
      "source": [
        "#Combining both healdlines and news content to form another column called \"content\"\n",
        "\n",
        "fake_real_data[\"content\"] = fake_real_data[\"title\"] + fake_real_data[\"text\"]\n",
        "print(fake_real_data.columns)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['title', 'text', 'subject', 'date', 'label', 'content'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNrgrD20-V8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "63fba9d2-c4d6-449a-d571-bedede3954a3"
      },
      "source": [
        "#Splitting dataset into train and test dataset \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    fake_real_data[\"content\"], fake_real_data['label'], test_size=0.3, random_state=1)\n",
        "\n",
        "print(\"Dimension of training data:\", X_train.shape)\n",
        "print(\"Dimension of test data:\", X_test.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension of training data: (31428,)\n",
            "Dimension of test data: (13470,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCdPwER4Hm-E",
        "colab_type": "text"
      },
      "source": [
        "### **Text Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXxLc8wdF3KL",
        "colab_type": "text"
      },
      "source": [
        "### **Exploratory Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dMByIwxF-4C",
        "colab_type": "text"
      },
      "source": [
        "### **Word Embeddings** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhbtgzgGNcL",
        "colab_type": "text"
      },
      "source": [
        "### **Classifiers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrwvShE8GRXq",
        "colab_type": "text"
      },
      "source": [
        "### **Performance Evaluation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpmxvf5gHyiG",
        "colab_type": "text"
      },
      "source": [
        "## **Future Work**\n",
        "\n",
        "I have used NLTK primarily for the analysis. It will be continuously upated to give a comparison of preprocessing the text using NLTK and spaCy. Also, I intend to extend the notebook by implementing deep learning models for text classification. \n",
        "\n",
        "For more information: [GitHub](https://github.com/dhanyajothimani)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw4G02M9gBdU",
        "colab_type": "text"
      },
      "source": [
        "### **References** \n",
        "\n",
        "Examples and concepts compiled from various sources including Python package documentation, Data camp, and StackOverflow."
      ]
    }
  ]
}